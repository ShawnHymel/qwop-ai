{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6245fca-df28-43b4-b886-b199710201b5",
   "metadata": {},
   "source": [
    "# QWOP AI\n",
    "\n",
    "Direct screen capture for observation. Make sure QWOP is in Chrome and window is resized just before scroll bars appear. Place window in top-left of screen. Note that this project is intended to be run *locally* on a computer where you can easily set up the game window for screen capture.\n",
    "\n",
    "Based on the \"Build a Chrome Dino Game AI Model with Python\" video by Nicholas Renotte: https://www.youtube.com/watch?v=vahwuupy81A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dfbbf-b0bf-4937-bc51-bb17d6acc2b9",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You will need to install OpenCV. The easiest way to do this is from the Anaconda environment. The newest version might work, but 4.7.0 worked for this notebook.\n",
    "\n",
    "```\n",
    "conda install -c conda-forge opencv=4.7.0\n",
    "```\n",
    "\n",
    "Additionally, you will need to install Tesseract on your computer. For Windows, install the executable from here: https://github.com/UB-Mannheim/tesseract/wiki. Then, add the folder with *tesseract.exe* to your PATH. pytesseract is just a Python wrapper, and it requires tesseract to be on the system path.\n",
    "\n",
    "Finally, you will need to install PyTorch for Stable Baselines3 to work. It is highly recommended that you use the GPU-enabled version of PyTorch for faster training. Follow the directions here: https://pytorch.org/get-started/locally/. This notebook was tested with the following:\n",
    " * PyTorch v2.0.0\n",
    " * OS: Windows\n",
    " * Package: Conda\n",
    " * Language: Python\n",
    " * Compute platform: CUDA 11.7\n",
    "\n",
    "### Install packages\n",
    "\n",
    "Uncomment the following cells to install the required packages. Note the versions on some of them. Gymnasium and Stable-Baselines3 change constantly, so I tried to version-lock these packages, as these are known to work with this notebook. Starting with v2.0.0, Stable-Baselines3 will only support gymnasium (and drop gym support). See [here](https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.8.0) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88747836-3b50-46fb-bd84-0881481583d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install mss pynput pytesseract\n",
    "# !python -m pip install gymnasium==0.28.1\n",
    "# !python -m pip install stable-baselines3[extra]==2.0.0a1\n",
    "# !python -m pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5593f1-e172-4279-8554-cae1e994110e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5678e-92c8-42b3-9f43-855c2eba6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "\n",
    "# Screen capture\n",
    "from mss import mss\n",
    "\n",
    "# Sending commands (e.g. mouse/keyboard)\n",
    "import pynput\n",
    "\n",
    "# OpenCV for image manipulation\n",
    "import cv2\n",
    "\n",
    "# Optical character recognition (OCR)\n",
    "import pytesseract\n",
    "\n",
    "# Farama Foundation Gymnasium (fork of OpenAI gym)\n",
    "import gymnasium as gym\n",
    "\n",
    "# Reinforcement model modules\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import KVWriter, Logger\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Weights & Biases for remote logging\n",
    "import wandb\n",
    "\n",
    "# Other\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Dict, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d0b37-dbcd-44fe-bb09-19ffa3cf46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings\n",
    "\n",
    "# Game observation\n",
    "GAME_CROP = {\n",
    "    'top':430, \n",
    "    'left':300, \n",
    "    'width':480, \n",
    "    'height':320\n",
    "}\n",
    "\n",
    "# Image resize (minimum of 36x36 for default CnnPolicies)\n",
    "GAME_RESIZE_WIDTH = 36\n",
    "GAME_RESIZE_HEIGHT = 36\n",
    "\n",
    "# How big to make display image (0 for no display)\n",
    "DISP_SCALE_FACTOR = 8.0\n",
    "\n",
    "# Score observation\n",
    "SCORE_CROP = {\n",
    "    'top':395, \n",
    "    'left':342, \n",
    "    'width':380, \n",
    "    'height':42\n",
    "}\n",
    "\n",
    "# Game over screen\n",
    "DONE_CROP = {\n",
    "    'top':635, \n",
    "    'left':430, \n",
    "    'width':220, \n",
    "    'height':38}\n",
    "GAME_OVER_STRINGS = [\"press\"]\n",
    "\n",
    "# Action settings\n",
    "RESTART_MOUSE_POS = (300, 600)\n",
    "RESTART_KEY = 'r'\n",
    "ACTIONS_KEY_PRESS_TIME = 0.05\n",
    "ACTIONS_MAP = {\n",
    "    0:'no-op',\n",
    "    1:'q',\n",
    "    2:'w',\n",
    "    3:'o',\n",
    "    4:'p'\n",
    "}\n",
    "\n",
    "# CnnPolicy requires 8-bit unsigned integers for images\n",
    "DTYPE = np.uint8\n",
    "\n",
    "# Checkpoint config\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CHECKPOINT_FREQ = 10_000\n",
    "\n",
    "# Log config\n",
    "LOG_DIR = \"logs\"\n",
    "LOG_FREQ = 1_000\n",
    "\n",
    "# Weights & Biases configuratino\n",
    "WANDB_PROJECT = \"qwop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b2e8c-f906-45f7-bf9b-a2427e651df1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Screen Capture\n",
    "\n",
    "Figure out how to set the crops and OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1639d6-b4ea-4456-be68-b566f7a86530",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create screen capture object\n",
    "screen = mss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288f508-1fee-4b57-9aaf-fbf892c9c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do screen grab and preprocess (make sure runner and some distance ahead is visible)\n",
    "\n",
    "# Get screen grab and drop alpha channel\n",
    "game_img = screen.grab(GAME_CROP)\n",
    "game_img = np.array(game_img)[:, :, :3]\n",
    "\n",
    "# Convert to grayscale and resize\n",
    "game_img = cv2.cvtColor(game_img, cv2.COLOR_BGR2GRAY)\n",
    "game_img = cv2.resize(game_img, (GAME_RESIZE_WIDTH, GAME_RESIZE_HEIGHT))\n",
    "\n",
    "# Add channel dimension first (in case you want RGB later)\n",
    "game_img = np.reshape(game_img, (1, GAME_RESIZE_HEIGHT, GAME_RESIZE_WIDTH))\n",
    "\n",
    "# Show crop/resized\n",
    "print(f\"Shape: {game_img.shape}\")\n",
    "print(f\"Example row: {game_img[0, 0, :]}\")\n",
    "plt.imshow(game_img[0], cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "# Try showing image in new window\n",
    "if DISP_SCALE_FACTOR > 0:\n",
    "    disp_width = int(GAME_RESIZE_WIDTH * DISP_SCALE_FACTOR)\n",
    "    disp_height = int(GAME_RESIZE_HEIGHT * DISP_SCALE_FACTOR)\n",
    "    disp_img = cv2.resize(game_img[0], (disp_width, disp_height), interpolation=cv2.INTER_AREA)\n",
    "    cv2.namedWindow('Game Image')\n",
    "    cv2.imshow('Game Image', disp_img)\n",
    "    cv2.waitKey(2000)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171dfe46-e390-4ab9-9b06-6eccc2f37fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use OCR to get score\n",
    "\n",
    "# Get screen grab and drop alpha channel\n",
    "score_img = screen.grab(SCORE_CROP)\n",
    "score_img = np.array(score_img)[:, :, :3]\n",
    "\n",
    "# Convert to RGB\n",
    "score_img = cv2.cvtColor(score_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Do OCR to get distance traveled\n",
    "ocr_str = pytesseract.image_to_string(score_img).strip()\n",
    "score = 0.0\n",
    "if ocr_str:\n",
    "    score_str = ocr_str.split()[0]\n",
    "    try:\n",
    "        score = float(float(score_str))\n",
    "    except ValueError:\n",
    "        pass\n",
    "print(f\"OCR string: {ocr_str}\")\n",
    "print(f\"Score: {score}\")\n",
    "\n",
    "# Show image\n",
    "plt.imshow(score_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a960a6-cb52-4a89-a1e6-582d726e6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use OCR to get done screen\n",
    "\n",
    "# Get screen grab and drop alpha channel\n",
    "done_img = screen.grab(DONE_CROP)\n",
    "done_img = np.array(done_img)[:, :, :3]\n",
    "\n",
    "# Convert to RGB\n",
    "done_img = cv2.cvtColor(done_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Do OCR to see if game is over\n",
    "ocr_str = pytesseract.image_to_string(done_img).strip()\n",
    "done = False\n",
    "if ocr_str:\n",
    "    done_str = ocr_str.split()[0]\n",
    "    if done_str in GAME_OVER_STRINGS:\n",
    "        done = True\n",
    "print(f\"OCR string: {ocr_str}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# Show image\n",
    "plt.imshow(done_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99f0a7-3e42-4e17-9c8d-7a3a15e79db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Spaces\n",
    "\n",
    "The gymnasium Environment (Env) requires us to define the observation space and action space so it can know what kinds of data to expect when it interacts with the environment. For us, the observation space is the shape and bit width of the normalized, scaled screen capture (our runner and some space in front of them). The action space is all the available actions we can input into the game (q, w, o, p) along with doing nothing (no-op). We should also test the ability to restart the game (pressing space bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf83847-9d2a-4020-88b1-74cab6ff7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test observation space\n",
    "\n",
    "# Create an observation space\n",
    "observation_space = gym.spaces.Box(low=0,\n",
    "                                   high=255,\n",
    "                                   shape=(1, GAME_RESIZE_WIDTH, GAME_RESIZE_WIDTH),\n",
    "                                   dtype=DTYPE)\n",
    "\n",
    "# Randomly sample from it to show that it matches our expected input screen capture\n",
    "obs = observation_space.sample()\n",
    "print(f\"Shape: {obs.shape}\")\n",
    "print(f\"Example row: {obs[0, 0, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5e2a9-ca67-4532-8023-12f423c1ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test action space\n",
    "\n",
    "# Create an action space\n",
    "action_space = gym.spaces.Discrete(len(ACTIONS_MAP))\n",
    "\n",
    "# Sample from it to show that we can randomly generate a number corresponding to an action\n",
    "action = action_space.sample()\n",
    "print(f\"{action}: {ACTIONS_MAP[action]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74db97-d55e-4804-9414-b2328627d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test control interaction: reset game and do some random stuff\n",
    "\n",
    "# Create interaction objects\n",
    "keyboard = pynput.keyboard.Controller()\n",
    "mouse = pynput.mouse.Controller()\n",
    "\n",
    "# Move mouse to QWOP window and click to bring to focus\n",
    "mouse.position = RESTART_MOUSE_POS\n",
    "mouse.press(pynput.mouse.Button.left)\n",
    "mouse.release(pynput.mouse.Button.left)\n",
    "\n",
    "# Press 'r' to restart game\n",
    "keyboard.press(RESTART_KEY)\n",
    "keyboard.release(RESTART_KEY)\n",
    "\n",
    "# Sample randomly from our action space to control the game\n",
    "for _ in range(10):\n",
    "    action = action_space.sample()\n",
    "    if action > 0:\n",
    "        keyboard.press(ACTIONS_MAP[action])\n",
    "        time.sleep(0.1)\n",
    "        keyboard.release(ACTIONS_MAP[action])\n",
    "        time.sleep(0.1)\n",
    "    else:\n",
    "        time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779fc038-5ea9-4b97-83fc-19b20af1030b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build gym Environment\n",
    "\n",
    "Subclass gymnasium.Env to create a custom environment. Learn more here: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22b32b-7115-4a33-8299-bde8d40805a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGame(gym.Env):\n",
    "    \"\"\"\n",
    "    Subclass gymnasium Env class\n",
    "    \n",
    "    This is the gym wrapper class that allows our agent to interact with our environment. We need\n",
    "    to implement four main methods: step(), reset(), render(), and close(). We should also define\n",
    "    the action_space and observation space as class members.\n",
    "    \n",
    "    More information: https://gymnasium.farama.org/api/env/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the environment, action, and observation shapes. Optional timeout in seconds.\n",
    "    def __init__(self, timeout=0.0, disp_scale=0.0, show_fps=False):\n",
    "        \n",
    "        # Call superclass's constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # Env requires us to define the action space\n",
    "        self.action_space = gym.spaces.Discrete(len(ACTIONS_MAP))\n",
    "        \n",
    "        # Env requires us to define the observation space\n",
    "        self.observation_space = gym.spaces.Box(low=0,\n",
    "                                                high=255,\n",
    "                                                shape=(1, GAME_RESIZE_HEIGHT, GAME_RESIZE_WIDTH),\n",
    "                                                dtype=DTYPE)\n",
    "        \n",
    "        # Screen capture object\n",
    "        self.screen = mss()\n",
    "        \n",
    "        # Record total score between rounds (to calculate reward each step)\n",
    "        self.score = 0.0\n",
    "        \n",
    "        # Used to record the time\n",
    "        self.timeout = timeout\n",
    "        self.start_time = 0.0\n",
    "        if self.timeout > 0.0:\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "        # How much to scale the render window\n",
    "        self.disp_scale = disp_scale\n",
    "        \n",
    "        # Show FPS in render window\n",
    "        self.show_fps = show_fps\n",
    "        self.timestamp = time.time()\n",
    "\n",
    "        # Initialize game image\n",
    "        self.game_img = np.zeros((1, GAME_RESIZE_HEIGHT, GAME_RESIZE_WIDTH))\n",
    "        \n",
    "        # Create render window\n",
    "        cv2.namedWindow('Game Image')\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # What happens when you take a step in the game (e.g. each frame)\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Perform action (don't do anything for no-op)\n",
    "        if ACTIONS_MAP[action] != 'no-op':\n",
    "            keyboard.press(ACTIONS_MAP[action])\n",
    "            time.sleep(ACTIONS_KEY_PRESS_TIME)\n",
    "            keyboard.release(ACTIONS_MAP[action])\n",
    "        else:\n",
    "            time.sleep(ACTIONS_KEY_PRESS_TIME)\n",
    "            \n",
    "        # Get next observation and render\n",
    "        obs = self.get_observation()\n",
    "        \n",
    "        # Use distance as total score. Calculate score difference between this step and previous.\n",
    "        prev_score = self.score\n",
    "        self.score = self.get_score()\n",
    "        reward = self.score - prev_score\n",
    "        \n",
    "        # Check if done\n",
    "        terminated = self.get_done()\n",
    "        \n",
    "        # Check if we've exceeded the time limit\n",
    "        elapsed_time = 0.0\n",
    "        truncated = False\n",
    "        if not terminated and self.timeout > 0.0:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            if elapsed_time >= self.timeout:\n",
    "                truncated = True\n",
    "        \n",
    "        # Return auxiliary information for debugging\n",
    "        info = {'score': self.score, 'time': elapsed_time}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    # Visualize the game using OpenCV\n",
    "    def render(self):\n",
    "        if self.disp_scale > 0:\n",
    "            \n",
    "            # Resize our game image to something that can be easily seen\n",
    "            disp_width = int(GAME_RESIZE_WIDTH * DISP_SCALE_FACTOR)\n",
    "            disp_height = int(GAME_RESIZE_HEIGHT * DISP_SCALE_FACTOR)\n",
    "            disp_img = cv2.resize(self.game_img[0], (disp_width, disp_height), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # Add FPS counter to image\n",
    "            if self.show_fps:\n",
    "                now = time.time()\n",
    "                fps = 1 / (now - self.timestamp)\n",
    "                self.timestamp = now\n",
    "                disp_img = cv2.putText(disp_img, \n",
    "                                       f\"fps: {fps:.1f}\", \n",
    "                                       (10, 25), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                                       1, \n",
    "                                       (255), \n",
    "                                       2, \n",
    "                                       cv2.LINE_AA)\n",
    "                \n",
    "            # Draw and wait 1 ms\n",
    "            cv2.imshow('Game Image', disp_img)\n",
    "            cv2.waitKey(1)\n",
    "    \n",
    "    # Restart the game\n",
    "    def reset(self):\n",
    "        \n",
    "        # Wait, move mouse to game window, click for focus\n",
    "        time.sleep(0.5)\n",
    "        mouse.position = RESTART_MOUSE_POS\n",
    "        mouse.press(pynput.mouse.Button.left)\n",
    "        mouse.release(pynput.mouse.Button.left)\n",
    "        \n",
    "        # Press 'space' to restart game\n",
    "        keyboard.press(RESTART_KEY)\n",
    "        time.sleep(ACTIONS_KEY_PRESS_TIME)\n",
    "        keyboard.release(RESTART_KEY)\n",
    "        \n",
    "        # Reset score and time\n",
    "        self.score = 0.0\n",
    "        if self.timeout > 0.0:\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        # Get first observation of new game\n",
    "        obs = self.get_observation()\n",
    "        \n",
    "        # Return auxiliary information for debugging\n",
    "        info = {'score': self.score, 'time': 0.0}\n",
    "        \n",
    "        return obs, info\n",
    "    \n",
    "    # Close down the game\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    # Get the part of the observation of the game that we want (e.g. crop, resize)\n",
    "    def get_observation(self):\n",
    "        \n",
    "        # Get screen grab and drop alpha channel\n",
    "        game_img = screen.grab(GAME_CROP)\n",
    "        game_img = np.array(game_img, dtype=DTYPE)[:, :, :3]\n",
    "\n",
    "        # Convert to grayscale and resize\n",
    "        game_img = cv2.cvtColor(game_img, cv2.COLOR_BGR2GRAY)\n",
    "        game_img = cv2.resize(game_img, (GAME_RESIZE_WIDTH, GAME_RESIZE_HEIGHT))\n",
    "        \n",
    "        # Add channel dimension first (in case you want RGB later)\n",
    "        game_img = np.reshape(game_img, (1, GAME_RESIZE_HEIGHT, GAME_RESIZE_WIDTH))\n",
    "        \n",
    "        # Render\n",
    "        self.game_img = game_img\n",
    "        self.render()\n",
    "        \n",
    "        return game_img\n",
    "    \n",
    "    # Get the distance ran to use as a total score and to calculate rewards\n",
    "    def get_score(self):\n",
    "        \n",
    "        # Get screen grab and drop alpha channel\n",
    "        score_img = screen.grab(SCORE_CROP)\n",
    "        score_img = np.array(score_img)[:, :, :3]\n",
    "\n",
    "        # Convert to RGB\n",
    "        score_img = cv2.cvtColor(score_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Do OCR to get distance traveled\n",
    "        ocr_str = pytesseract.image_to_string(score_img).strip()\n",
    "        score = 0.0\n",
    "        if ocr_str:\n",
    "            score_str = ocr_str.split()[0]\n",
    "            try:\n",
    "                score = float(float(score_str))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    # Get the done text using OCR\n",
    "    def get_done(self):\n",
    "        \n",
    "        # Get screen grab and drop alpha channel\n",
    "        done_img = screen.grab(DONE_CROP)\n",
    "        done_img = np.array(done_img)[:, :, :3]\n",
    "\n",
    "        # Convert to RGB\n",
    "        done_img = cv2.cvtColor(done_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Do OCR to see if game is over\n",
    "        ocr_str = pytesseract.image_to_string(done_img).strip()\n",
    "        done = False\n",
    "        if ocr_str:\n",
    "            done_str = ocr_str.split()[0]\n",
    "            if done_str in GAME_OVER_STRINGS:\n",
    "                done = True\n",
    "                \n",
    "        return done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750eb59-9fea-45de-a33a-901ef346f079",
   "metadata": {},
   "source": [
    "## Test gym Environment\n",
    "\n",
    "It's always a good idea to test your environment wrapper before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67317941-9de8-424a-a330-888c9321d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment with a timeout\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = WebGame(timeout=5.0, disp_scale=DISP_SCALE_FACTOR, show_fps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0836d4c-c364-4045-81b7-731a0374e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test environment wrapper methods\n",
    "\n",
    "# Get current score\n",
    "print(f\"Score: {env.get_score()}\")\n",
    "\n",
    "# See if game is done\n",
    "print(f\"Done: {env.get_done()}\")\n",
    "\n",
    "# Get the current observation\n",
    "obs = env.get_observation()\n",
    "print(f\"Shape: {obs.shape}\")\n",
    "plt.imshow(obs[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b621e-15d1-4fb3-a89c-7ef9a340b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test game loop\n",
    "\n",
    "debug = False\n",
    "\n",
    "# Do a few game loops\n",
    "for ep in range(3):\n",
    "    \n",
    "    # Initialize game\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Do a single game loop\n",
    "    while not terminated and not truncated:\n",
    "        obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "        if debug:\n",
    "            print(f\"Score: {info['score']}, Time: {info['time']}, \"\n",
    "                  f\"Terminated: {terminated}, Truncated: {truncated}\")\n",
    "        \n",
    "    # Show results\n",
    "    print(f\"Total reward for episode {ep} is {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad13a7-8912-4e61-83d1-f53f6e739ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if game runs while lid is closed\n",
    "# Should receive consistent ~ -1 m scores\n",
    "num_tests = 5\n",
    "for ep in range(num_tests):\n",
    "    \n",
    "    # Initialize game\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    action = 4\n",
    "    \n",
    "    # Do a single game loop\n",
    "    while not terminated and not truncated:\n",
    "        \n",
    "        # Repeatedly press 'q' and 'p' to fall backward\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if action == 1:\n",
    "            action = 4\n",
    "        else:\n",
    "            action = 1\n",
    "        if debug:\n",
    "            print(f\"Score: {info['score']}, Time: {info['time']}, \"\n",
    "                  f\"Terminated: {terminated}, Truncated: {truncated}\")\n",
    "        \n",
    "    # Show results\n",
    "    print(f\"Total reward for episode {ep} is {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4077fec-0a83-48e7-8b2c-4b279790f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final environment check to make sure it works with Stable-Baselines3\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7fac-1a00-49f8-ac84-1e6c8ef98abf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checkpoint callback\n",
    "\n",
    "We'll create a custom callback for Stable Baselines3 that saves the model every n steps. This helps us recover from an error, disconnect, or power loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7a9b2-4109-4ec5-9839-454fda10ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpointCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Save the model every ``check_freq`` steps\n",
    "    \n",
    "    More information: https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, check_freq, save_dir, verbose=1):\n",
    "        super(SaveCheckpointCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "    # Create directory for saving the models\n",
    "    def _init_callback(self):\n",
    "        if self.save_dir is not None:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "    # Save model every check_freq steps\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_dir, f\"model_{self.n_calls}\")\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3e2aa-535a-4601-bb8d-ddba38cdc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoint callback\n",
    "checkpoint_callback = SaveCheckpointCallback(\n",
    "    check_freq=CHECKPOINT_FREQ, \n",
    "    save_dir=CHECKPOINT_DIR,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99571557-1569-46a3-ad12-c7a4c83e7cfa",
   "metadata": {},
   "source": [
    "## Custom logger\n",
    "\n",
    "I'm going to use Weights & Biases for logging so I can view the training progress remotely. See the [Logger documentation](https://stable-baselines3.readthedocs.io/en/master/common/logger.html) and [integration examples](https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst) for how to create a custom writer.\n",
    "\n",
    "Feel free to use a different method for logging, such as TensorBoard. See [here](https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html) for how to set up TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122520ff-e391-4e89-bc20-ff7bb453bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBWriter(KVWriter):\n",
    "    \"\"\"\n",
    "    Log metrics to Weights & Biases when called by .learn()\n",
    "    \n",
    "    More info: https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/logger.html#KVWriter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize run\n",
    "    def __init__(self, run, verbose=1):\n",
    "        super().__init__()\n",
    "        self.run = run\n",
    "        self.verbose = verbose\n",
    "\n",
    "    # Write metrics to W&B project\n",
    "    def write(self, \n",
    "              key_values: Dict[str, Any], \n",
    "              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], \n",
    "              step: int = 0) -> None:\n",
    "        log_dict = {}\n",
    "        \n",
    "        # Go through each key/value pairs\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())):\n",
    "            \n",
    "            if self.verbose >= 2:\n",
    "                print(f\"step={step} | {key} : {value} ({type(value)})\")\n",
    "            \n",
    "            # Skip excluded items\n",
    "            if excluded is not None and \"wandb\" in excluded:\n",
    "                continue\n",
    "                \n",
    "            # Log integers and floats\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    wandb.log(data={key: value}, step=step)\n",
    "                    log_dict[key] = value\n",
    "                \n",
    "        # Print to console\n",
    "        if self.verbose >= 1:\n",
    "            print(f\"Log for steps={step}\")\n",
    "            print(f\"--------------\")\n",
    "            for (key, value) in sorted(log_dict.items()):\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "                \n",
    "    # Close the W&B run\n",
    "    def close(self) -> None:\n",
    "        self.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db30a32-0807-4f5a-b38c-265aa44bb775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c03a8-0b04-46a5-accb-e301009dceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new W&B run\n",
    "config = {}\n",
    "dt = datetime.datetime.now(datetime.timezone.utc)\n",
    "dt = dt.replace(microsecond=0, tzinfo=None)\n",
    "run = wandb.init(project=WANDB_PROJECT, name=str(dt), config=config)\n",
    "\n",
    "# Print run info\n",
    "print(f\"WandB run ID: {run.id}\")\n",
    "print(f\"WandB run name: {run.name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97feb0d2-9409-4fbd-9388-89e53b37882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom logger with our custom writer\n",
    "wandb_writer = WandBWriter(run, verbose=0)\n",
    "loggers = Logger(\n",
    "    folder=None,\n",
    "    output_formats=[wandb_writer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b5922-6030-4abb-8296-0d3dc73ce7f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67218cc3-c737-4205-b030-669711ddae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hparams = {\n",
    "    \"episode_timeout\": 600,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"steps_per_update\": 512,\n",
    "    \"total_timesteps\": 20_000,\n",
    "    \"num_actions\": len(ACTIONS_MAP),\n",
    "    \"notes\": \"Increased episode timeout, as previous model was just falling forward\"\n",
    "}\n",
    "\n",
    "# Log hyperparameters to W&B\n",
    "wandb.config.update(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db5418-3b56-4ec6-89d0-66a984b7d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment with a timeout for training\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = WebGame(timeout=hparams[\"episode_timeout\"], disp_scale=DISP_SCALE_FACTOR, show_fps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcdd2-9592-4ccc-90a2-160aaaefee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# More information: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "model = PPO('CnnPolicy', \n",
    "            env, \n",
    "            verbose=0,\n",
    "            learning_rate=hparams[\"learning_rate\"], \n",
    "            n_steps=hparams[\"steps_per_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfd7bd-f03b-474b-9d3c-562ceb3234aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choo choo train!\n",
    "# total_timesteps: take at least this many steps, will st op on multiple of n_steps in PPO\n",
    "# Host tensorboard on LAN: tensorboard --host 0.0.0.0 --logdir logs\\PPO_n\n",
    "# (But note that we're using Weights & Biases for remote logging instead)\n",
    "# About logs: https://stable-baselines3.readthedocs.ioqqp/en/master/common/logger.html\n",
    "# About plots: https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2\n",
    "model.set_logger(loggers)\n",
    "model.learn(total_timesteps=hparams[\"total_timesteps\"], \n",
    "            callback=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b301248-f7ba-4caa-b401-7c205924ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close W&B run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ba961-be85-409a-9d67-d9ffbfb10d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save(\"qwop_model_v01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f1f3b-1f4e-4853-b8b6-5c2f22b854db",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa5e16-52f2-4aec-b10c-466114ca6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment for testing\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = WebGame(timeout=60.0, disp_scale=DISP_SCALE_FACTOR, show_fps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c3e2e-3d47-4174-bf18-fd6f9f7a8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (use final model or any checkpoint)\n",
    "model = PPO.load(\n",
    "    os.path.join(\".\", \"qwop_model_v01\"), \n",
    "    env=env, \n",
    "    print_system_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e74383-8d19-455b-8137-13e8b62a60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play game\n",
    "for ep in range(3):\n",
    "    \n",
    "    # Reset game\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    # Perform actions based on observation and accumulate reward\n",
    "    while not terminated and not truncated:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "    # Print total reward at the end of the episode\n",
    "    print(f\"Episode {ep} | Steps: {step_count}, Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dea137-15f1-4954-89e8-fcd16269898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're done! Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754caf2c-d149-4e96-ae34-31748c7d4ca9",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    " * See if training is actually happening with lid closed\n",
    " * Properly close out wandb run\n",
    " * Add frame stacking\n",
    " * Add button combos (e.g. qw, qo, qp, wo, wp, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466edd4-5f1d-4447-ab2c-ebfbb96ff168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
